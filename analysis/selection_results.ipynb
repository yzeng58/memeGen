{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root_dir)\n",
    "from helper import read_json\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_llm_name = \"gemini-1.5-pro\"\n",
    "eval_model = \"gemini-1.5-pro\"\n",
    "peft_variant = \"pretrained\"\n",
    "eval_prompt_name = \"standard\"\n",
    "result_dir = f\"{root_dir}/results/generation/british_complaints/{gen_llm_name}/stable-diffusion-3-medium-diffusers/selective/{eval_model}/{peft_variant}/{eval_prompt_name}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_llm_name = \"gemini-1.5-pro\"\n",
    "eval_model = \"Qwen2-VL-72B-Instruct\"\n",
    "peft_variant = \"qlora_llm_meme_Qwen2-VL-72B-Instruct_multimodal_pairwise_standard_0_shot_train_3_epochs_0.0001_lr\"\n",
    "eval_prompt_name = \"standard\"\n",
    "result_dir = f\"{root_dir}/results/generation/british_complaints/{gen_llm_name}/stable-diffusion-3-medium-diffusers/selective/{eval_model}/{peft_variant}/{eval_prompt_name}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_image_paths = []\n",
    "for i in range(1, 51):\n",
    "    result_file = f\"{result_dir}/{i}.json\"\n",
    "    result = read_json(result_file)\n",
    "\n",
    "    max_score = -1\n",
    "    best_prompt = None\n",
    "    for key in result:\n",
    "        match = re.search(r'stable-diffusion-3-medium-diffusers/([^/]+)/standard/output/', key)\n",
    "\n",
    "        prompt_type = match.group(1)\n",
    "\n",
    "        if prompt_type in [\"standard\", \"lot\"]: continue \n",
    "        score = result[key]\n",
    "        \n",
    "        if score >= max_score:\n",
    "            max_score = score\n",
    "            best_prompt = prompt_type\n",
    "    best_image_paths.append(f\"{gen_llm_name}/stable-diffusion-3-medium-diffusers/{best_prompt}/standard/meme/{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{root_dir}/resources/datasets/llm_meme/dataset_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24666666666666667"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_funny, n_valid = 0, 0\n",
    "for path in best_image_paths:\n",
    "    rows = df[df[\"image_path\"] == path]\n",
    "    if len(rows): \n",
    "        for i, row in rows.iterrows():\n",
    "            n_valid += 1\n",
    "            n_funny += row[\"Q5_option\"] >= 4\n",
    "    else: \n",
    "        label = 0\n",
    "    n_funny += label\n",
    "n_funny / n_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
